{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn, einsum\nimport numpy as np\nfrom einops import rearrange, repeat\nimport torch.nn as nn\nimport torch.nn.functional as F\nwarnings.filterwarnings(\"ignore\")\nplt.style.use(\"seaborn-darkgrid\")\nsns.set_context(\"paper\", font_scale=1.4)\n\nid_map = {\n    0: (0, 0, 0), # unlabelled\n    1: (111, 74,  0), #static\n    2: ( 81,  0, 81), #ground\n    3: (128, 64,127), #road\n    4: (244, 35,232), #sidewalk\n    5: (250,170,160), #parking\n    6: (230,150,140), #rail track\n    7: (70, 70, 70), #building\n    8: (102,102,156), #wall\n    9: (190,153,153), #fence\n    10: (180,165,180), #guard rail\n    11: (150,100,100), #bridge\n    12: (150,120, 90), #tunnel\n    13: (153,153,153), #pole\n    14: (153,153,153), #polegroup\n    15: (250,170, 30), #traffic light\n    16: (220,220,  0), #traffic sign\n    17: (107,142, 35), #vegetation\n    18: (152,251,152), #terrain\n    19: ( 70,130,180), #sky\n    20: (220, 20, 60), #person\n    21: (255,  0,  0), #rider\n    22: (  0,  0,142), #car\n    23: (  0,  0, 70), #truck\n    24: (  0, 60,100), #bus\n    25: (  0,  0, 90), #caravan\n    26: (  0,  0,110), #trailer\n    27: (  0, 80,100), #train\n    28: (  0,  0,230), #motorcycle\n    29: (119, 11, 32), #bicycle\n    30: (  0,  0,142) #license plate \n}\n\ncategory_map = {\n    0: 0,\n    1: 0,\n    2: 0,\n    3: 1,\n    4: 1,\n    5: 1,\n    6: 1,\n    7: 2,\n    8: 2,\n    9: 2,\n    10: 2,\n    11: 2,\n    12: 2,\n    13: 3,\n    14: 3,\n    15: 3,\n    16: 3,\n    17: 4,\n    18: 4,\n    19: 5,\n    20: 6,\n    21: 6,\n    22: 7,\n    23: 7,\n    24: 7,\n    25: 7,\n    26: 7,\n    27: 7,\n    28: 7,\n    29: 7,\n    30: 7\n}\n\n#num_classes = len(id_map.keys())\n\ndef get_fcn_model():\n    num_classes = len(id_map.keys())\n    # instantiate a Keras tensor\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    #First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f6)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f10)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f14)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n\n    \n    # First Upsample\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(f18)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    #c20 = tf.keras.layers.Concatenate()([d19, f14])\n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(d19)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    \n    # Second Upsample\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(f22)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    #c24 = tf.keras.layers.Concatenate()([d23, f10])\n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d23)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    \n    # Third Upsample\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(f26)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    #c28 = tf.keras.layers.Concatenate()([d27, f6])\n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d27)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    \n    #Forth Upsample\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(f30)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    #c32 = tf.keras.layers.Concatenate()([d31, f2])\n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d31)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    \n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(f34)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n\n\ndef get_unet_model():\n    num_classes = len(id_map.keys())\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    #First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f6)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f10)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f14)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n\n    \n    # First Upsample\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(f18)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, f14])\n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    \n    # Second Upsample\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(f22)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, f10])\n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    \n    # Third Upsample\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(f26)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, f6])\n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    \n    #Forth Upsample\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(f30)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, f2])\n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    \n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(f34)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n\n\n\nfrom tensorflow.keras import models, layers, regularizers\nfrom tensorflow.keras import backend as K\n\n\n#convolutional block\ndef conv_block(x, kernelsize, filters, dropout, batchnorm=False): \n    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(x)\n    if batchnorm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation(\"relu\")(conv)\n    if dropout > 0:\n        conv = layers.Dropout(dropout)(conv)\n    conv = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding=\"same\")(conv)\n    if batchnorm is True:\n        conv = layers.BatchNormalization(axis=3)(conv)\n    conv = layers.Activation(\"relu\")(conv)\n    return conv\n\n\n#residual convolutional block\ndef res_conv_block(x, kernelsize, filters, dropout, batchnorm=False):\n    conv1 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(x)\n    if batchnorm is True:\n        conv1 = layers.BatchNormalization(axis=3)(conv1)\n    conv1 = layers.Activation('relu')(conv1)    \n    conv2 = layers.Conv2D(filters, (kernelsize, kernelsize), kernel_initializer='he_normal', padding='same')(conv1)\n    if batchnorm is True:\n        conv2 = layers.BatchNormalization(axis=3)(conv2)\n        conv2 = layers.Activation(\"relu\")(conv2)\n    if dropout > 0:\n        conv2 = layers.Dropout(dropout)(conv2)\n        \n    #skip connection    \n    shortcut = layers.Conv2D(filters, kernel_size=(1, 1), kernel_initializer='he_normal', padding='same')(x)\n    if batchnorm is True:\n        shortcut = layers.BatchNormalization(axis=3)(shortcut)\n    shortcut = layers.Activation(\"relu\")(shortcut)\n    respath = layers.add([shortcut, conv2])       \n    return respath\n\n\n#gating signal for attention unit\ndef gatingsignal(input, out_size, batchnorm=False):\n    x = layers.Conv2D(out_size, (1, 1), padding='same')(input)\n    if batchnorm:\n        x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    return x\n\n#attention unit/block based on soft attention\ndef attention_block(x, gating, inter_shape):\n    shape_x = K.int_shape(x)\n    shape_g = K.int_shape(gating)\n    theta_x = layers.Conv2D(inter_shape, (2, 2), strides=(2, 2), kernel_initializer='he_normal', padding='same')(x) \n    shape_theta_x = K.int_shape(theta_x)\n    phi_g = layers.Conv2D(inter_shape, (1, 1), kernel_initializer='he_normal', padding='same')(gating)\n    upsample_g = layers.Conv2DTranspose(inter_shape, (3, 3), strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]), kernel_initializer='he_normal', padding='same')(phi_g)\n    concat_xg = layers.add([upsample_g, theta_x])\n    act_xg = layers.Activation('relu')(concat_xg)\n    psi = layers.Conv2D(1, (1, 1), kernel_initializer='he_normal', padding='same')(act_xg)\n    sigmoid_xg = layers.Activation('sigmoid')(psi)\n    shape_sigmoid = K.int_shape(sigmoid_xg)\n    upsample_psi = layers.UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg) \n    upsample_psi = layers.Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={'repnum': shape_x[3]})(upsample_psi)                          \n    y = layers.multiply([upsample_psi, x])\n    result = layers.Conv2D(shape_x[3], (1, 1), kernel_initializer='he_normal', padding='same')(y)\n    attenblock = layers.BatchNormalization()(result)\n    return attenblock\n\n\n\n\ndef get_resunet_model():\n    num_classes = len(id_map.keys())\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    #First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    \n    #Residual connection + Addition\n    resid1=tf.keras.layers.Conv2D(128, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    resid_bn1= tf.keras.layers.BatchNormalization()(resid1)\n    out1= tf.keras.layers.add([resid_bn1,f6]) \n    \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out1)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    \n    #Residual connection + Addition\n    resid2=tf.keras.layers.Conv2D(256, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    resid_bn2= tf.keras.layers.BatchNormalization()(resid2)\n    out2= tf.keras.layers.add([resid_bn2,f10]) \n    \n    \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out2)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    \n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    \n    #Residual connection + Addition\n    resid3=tf.keras.layers.Conv2D(512, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    resid_bn3=tf.keras.layers.BatchNormalization()(resid3)\n    out3= tf.keras.layers.add([resid_bn3,f14]) \n    \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out3)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n    \n    #Residual connection + Addition\n    resid4=tf.keras.layers.Conv2D(1024, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    resid_bn4=tf.keras.layers.BatchNormalization()(resid4)\n    out4= tf.keras.layers.add([resid_bn4,f18]) \n    \n    \n    \n    \n    # First Upsample\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(out4)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, f14])\n    \n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    \n    #Residual connection + Addition\n    resid5=tf.keras.layers.Conv2D(512, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c20)\n    resid_bn5=tf.keras.layers.BatchNormalization()(resid5)\n    out5= tf.keras.layers.add([resid_bn5,f22])\n    \n    \n    # Second Upsample\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(out5)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, f10])\n    \n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    \n    #Residual connection + Addition\n    resid6=tf.keras.layers.Conv2D(256, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    resid_bn6=tf.keras.layers.BatchNormalization()(resid6)\n    out6= tf.keras.layers.add([resid_bn6,f26])\n    \n    # Third Upsample\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(out6)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, f6])\n    \n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    \n    #Residual connection + Addition\n    resid7=tf.keras.layers.Conv2D(128, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    resid_bn7=tf.keras.layers.BatchNormalization()(resid7)\n    out7= tf.keras.layers.add([resid_bn7,f30])\n    \n    #Forth Upsample\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(out7)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, f2])\n    \n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    \n    #Residual connection + Addition\n    resid8=tf.keras.layers.Conv2D(64, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    resid_bn8=tf.keras.layers.BatchNormalization()(resid8)\n    out8= tf.keras.layers.add([resid_bn8,f34])\n    \n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(out8)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n\n\n\ndef get_unet_attn_model():\n    batchnorm=True\n    num_classes = len(id_map.keys())\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    #First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f6)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f10)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f14)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n\n    \n    # First Upsample\n    gating_5 = gatingsignal(f18, 512, batchnorm)\n    att_5 = attention_block(f14, gating_5, 512)\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(f18)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, att_5])\n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    \n    # Second Upsample\n    gating_4 = gatingsignal(f22, 256, batchnorm)\n    att_4 = attention_block(f10, gating_4, 256)\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(f22)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, att_4])\n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    \n    # Third Upsample\n    gating_3 = gatingsignal(f26, 128, batchnorm)\n    att_3 = attention_block(f6, gating_3, 128)\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(f26)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, att_3])\n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    \n    #Forth Upsample\n    gating_2 = gatingsignal(f30, 64, batchnorm)\n    att_2 = attention_block(f2, gating_2, 64)\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(f30)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, att_2])\n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    \n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(f34)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n\n\n\n\ndef get_res_att_unet_model():\n    batchnorm=True\n    num_classes = len(id_map.keys())\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    # First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    #Residual connection + Addition\n    resid1=tf.keras.layers.Conv2D(64, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    resid_bn1= tf.keras.layers.BatchNormalization()(resid1)\n    out1= tf.keras.layers.add([resid_bn1,f2])    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out1)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    #Residual connection + Addition\n    resid2=tf.keras.layers.Conv2D(128, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    resid_bn2= tf.keras.layers.BatchNormalization()(resid2)\n    out2= tf.keras.layers.add([resid_bn2,f6]) \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out2)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    #Residual connection + Addition\n    resid3=tf.keras.layers.Conv2D(256, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    resid_bn3= tf.keras.layers.BatchNormalization()(resid3)\n    out3= tf.keras.layers.add([resid_bn3,f10]) \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out3)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    #Residual connection + Addition\n    resid4=tf.keras.layers.Conv2D(512, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    resid_bn4=tf.keras.layers.BatchNormalization()(resid4)\n    out4= tf.keras.layers.add([resid_bn4,f14]) \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(out4)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n    #Residual connection + Addition\n    resid5=tf.keras.layers.Conv2D(1024, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    resid_bn5=tf.keras.layers.BatchNormalization()(resid5)\n    out5= tf.keras.layers.add([resid_bn5,f18]) \n    \n    \n    \n    \n    # First Upsample\n    gating_5 = gatingsignal(out5, 512, batchnorm)\n    att_5 = attention_block(out4, gating_5, 512)\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(out5)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, f14])\n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    #Residual connection + Addition\n    resid6=tf.keras.layers.Conv2D(512, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c20)\n    resid_bn6=tf.keras.layers.BatchNormalization()(resid6)\n    out6= tf.keras.layers.add([resid_bn6,f22])\n    \n    \n    # Second Upsample\n    gating_4 = gatingsignal(out6, 256, batchnorm)\n    att_4 = attention_block(out3, gating_4, 256)\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(out6)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, f10])\n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    #Residual connection + Addition\n    resid7=tf.keras.layers.Conv2D(256, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    resid_bn7=tf.keras.layers.BatchNormalization()(resid7)\n    out7= tf.keras.layers.add([resid_bn7,f26])\n    \n    \n    # Third Upsample\n    gating_3 = gatingsignal(out7, 128, batchnorm)\n    att_3 = attention_block(out2, gating_3, 128)\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(out7)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, f6])\n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    #Residual connection + Addition\n    resid8=tf.keras.layers.Conv2D(128, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    resid_bn8=tf.keras.layers.BatchNormalization()(resid8)\n    out8= tf.keras.layers.add([resid_bn8,f30])\n    \n    #Forth Upsample\n    gating_2 = gatingsignal(out8, 64, batchnorm)\n    att_2 = attention_block(out1, gating_2, 64)\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(out8)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, f2])\n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    #Residual connection + Addition\n    resid9=tf.keras.layers.Conv2D(64, kernel_size = (1, 1), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    resid_bn9=tf.keras.layers.BatchNormalization()(resid9)\n    out9= tf.keras.layers.add([resid_bn9,f34])\n\n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(out9)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n\n\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, Concatenate, Conv2DTranspose, GlobalAveragePooling2D\nfrom keras.layers import Reshape\nfrom keras.layers import BatchNormalization, Activation\n#from keras.layers import AtrousConvolution2D\ndef create_aspp(input_tensor, n_filters):\n    # ASPP module with dilated convolutions at different rates\n    # and global average pooling\n\n    # Dilated convolution with rate=1\n    conv1x1_1 = Conv2D(n_filters, (1, 1), padding='same', kernel_initializer='he_normal')(input_tensor)\n\n    # Dilated convolution with rate=6\n    conv3x3_1 = Conv2D(n_filters, (3, 3), padding='same', dilation_rate=(6, 6), kernel_initializer='he_normal')(input_tensor)\n\n    # Dilated convolution with rate=12\n    conv3x3_2 = Conv2D(n_filters, (3, 3), padding='same', dilation_rate=(12, 12), kernel_initializer='he_normal')(input_tensor)\n\n    # Dilated convolution with rate=18\n    conv3x3_3 = Conv2D(n_filters, (3, 3), padding='same', dilation_rate=(18, 18), kernel_initializer='he_normal')(input_tensor)\n\n    # Global average pooling\n    avg_pool = GlobalAveragePooling2D()(input_tensor)\n    avg_pool = Reshape((1, 1, n_filters))(avg_pool)\n    avg_pool = Conv2D(n_filters, (1, 1), padding='same', kernel_initializer='he_normal')(avg_pool)\n    avg_pool = UpSampling2D(size=(input_tensor.shape[1], input_tensor.shape[2]), interpolation='bilinear')(avg_pool)\n\n    # Concatenate all paths\n    concatenated = Concatenate()([conv1x1_1, conv3x3_1, conv3x3_2, conv3x3_3, avg_pool])\n    concatenated = Conv2D(n_filters, (1, 1), padding='same', kernel_initializer='he_normal')(concatenated)\n    concatenated = BatchNormalization()(concatenated)\n    concatenated = Activation('relu')(concatenated)\n\n    return concatenated\n\n\n\n\ndef get_unet_aspp_model():\n    num_classes = len(id_map.keys())\n    inputs = tf.keras.layers.Input(shape = [128, 128, 3])\n    \n    #First Downsample\n    f1 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b1)    # Used later for residual connection\n    \n    m3 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n    \n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b5)    # Used later for residual connection\n    \n    m7 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f6)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n    \n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b9)    # Used later for residual connection\n    \n    m11 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f10)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n    \n    #Forth Downsample\n    f13 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b13)    # Used later for residual connection\n    \n    m15 = tf.keras.layers.MaxPooling2D(pool_size = (2, 2), strides = 2)(f14)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n    \n    #Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b17)\n    \n    \n    #ASPP\n    aspp = create_aspp(f18, 1024)\n    f18 = tf.keras.layers.Dropout(0.2)(aspp)\n    \n    # First Upsample\n    m19 = tf.keras.layers.UpSampling2D(size = (2, 2))(f18)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, f14])\n    f21 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1 ,activation = \"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b21)\n    \n    # Second Upsample\n    m23 = tf.keras.layers.UpSampling2D(size = (2, 2))(f22)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, f10])\n    f25 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b25)\n    \n    # Third Upsample\n    m27 = tf.keras.layers.UpSampling2D(size = (2, 2))(f26)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, f6])\n    f29 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b29)\n    \n    #Forth Upsample\n    m31 = tf.keras.layers.UpSampling2D(size = (2, 2))(f30)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, f2])\n    f33 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"relu\")(b33)\n    \n    # Output Layer\n    outputs = tf.keras.layers.Conv2D(num_classes, kernel_size = (3, 3), padding = \"same\", strides = 1, activation = \"softmax\")(f34)\n    \n    model = tf.keras.Model(inputs = [inputs], outputs = [outputs])\n    return model\n\n#Swin Transformer\n# function definitions for SWIN Transformer\nclass CyclicShift(nn.Module):\n    def __init__(self, displacement):\n        super().__init__()\n        self.displacement = displacement\n\n    def forward(self, x):\n        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\ndef create_mask(window_size, displacement, upper_lower, left_right):\n    mask = torch.zeros(window_size ** 2, window_size ** 2)\n\n    if upper_lower:\n        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n\n    if left_right:\n        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n        mask[:, -displacement:, :, :-displacement] = float('-inf')\n        mask[:, :-displacement, :, -displacement:] = float('-inf')\n        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n\n    return mask\n\n\ndef get_relative_distances(window_size):\n    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n    distances = indices[None, :, :] - indices[:, None, :]\n    return distances\n\n\nclass WindowAttention(nn.Module):\n    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n        super().__init__()\n        inner_dim = head_dim * heads\n\n        self.heads = heads\n        self.scale = head_dim ** -0.5\n        self.window_size = window_size\n        self.relative_pos_embedding = relative_pos_embedding\n        self.shifted = shifted\n\n        if self.shifted:\n            displacement = window_size // 2\n            self.cyclic_shift = CyclicShift(-displacement)\n            self.cyclic_back_shift = CyclicShift(displacement)\n            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n                                                             upper_lower=True, left_right=False), requires_grad=False)\n            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n                                                            upper_lower=False, left_right=True), requires_grad=False)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n\n        if self.relative_pos_embedding:\n            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n        else:\n            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n\n        self.to_out = nn.Linear(inner_dim, dim)\n\n    def forward(self, x):\n        if self.shifted:\n            x = self.cyclic_shift(x)\n\n        b, n_h, n_w, _, h = *x.shape, self.heads\n\n        qkv = self.to_qkv(x).chunk(3, dim=-1)\n        nw_h = n_h // self.window_size\n        nw_w = n_w // self.window_size\n\n        q, k, v = map(\n            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n\n        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n\n        if self.relative_pos_embedding:\n            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n        else:\n            dots += self.pos_embedding\n\n        if self.shifted:\n            dots[:, :, -nw_w:] += self.upper_lower_mask\n            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n\n        attn = dots.softmax(dim=-1)\n\n        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n        out = self.to_out(out)\n\n        if self.shifted:\n            out = self.cyclic_back_shift(out)\n        return out\n\n\nclass SwinBlock(nn.Module):\n    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n        super().__init__()\n        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n                                                                     heads=heads,\n                                                                     head_dim=head_dim,\n                                                                     shifted=shifted,\n                                                                     window_size=window_size,\n                                                                     relative_pos_embedding=relative_pos_embedding)))\n        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n\n    def forward(self, x):\n        x = self.attention_block(x)\n        x = self.mlp_block(x)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    def __init__(self, in_channels, out_channels, downscaling_factor):\n        super().__init__()\n        self.downscaling_factor = downscaling_factor\n        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n    def forward(self, x):\n        b, c, h, w = x.shape\n        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n        x = self.linear(x)\n        #print(\"PMx.shape: \",x.shape)\n        return x\n\nfrom einops import rearrange\n\nclass PatchExpanding(nn.Module):\n    # def __init__(self, in_channels, out_channels, dim_scale=2, norm_layer=nn.LayerNorm):\n    def __init__(self, in_channels, out_channels, upscaling_factor):    \n        super().__init__()\n        self.expand = nn.Linear(in_channels, in_channels*2, bias=False) if upscaling_factor == 2 else nn.Identity()\n    def forward(self, x):\n        B, C, H, W = x.shape\n        #print(\"x.shape: \",x.shape)\n        x = x.view(B, C, H*W)\n        x = x.permute(0, 2, 1)\n        #print(\"x.shape: \",x.shape)\n        x = self.expand(x)\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        x = x.view(B, H, W, C)\n        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C // 4)\n        x = x.view(B, -1, C // 4)\n        # x = self.norm(x)\n        B, L, C = x.shape\n        #print(\"L:\",L)\n        x = x.view(B, int(L**(0.5)), int(L**(0.5)),C)\n        #print(\"x.shape: \",x.shape)\n        return x\n\nclass PatchFinalExpanding(nn.Module):\n    # def __init__(self, in_channels, out_channels, dim_scale=2, norm_layer=nn.LayerNorm):\n    def __init__(self, in_channels, out_channels, upscaling_factor):    \n        super().__init__()\n        self.upscaling_factor = upscaling_factor\n        self.in_channels = in_channels\n        self.out_channels= out_channels\n        self.expand = nn.Linear(in_channels, in_channels*16, bias=False)\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        #print(\"x.shape: \",x.shape)\n        x = x.view(B, C, H*W)\n        x = x.permute(0, 2, 1)\n        #print(\"x.shape: \",x.shape)\n        x = self.expand(x)\n        \n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        \n        x = x.view(B, H, W, C)\n        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.upscaling_factor, p2=self.upscaling_factor, c=C // (self.upscaling_factor**2))\n        x = x.view(B, -1, self.in_channels)\n        # x = self.norm(x)\n        B, L, C = x.shape\n        #print(\"L:\",L)\n        x = x.view(B, int(L**(0.5)), int(L**(0.5)),C)\n        #print(\"x.shape: \",x.shape)\n        return x\n    \nclass StageModule(nn.Module):\n    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n                 relative_pos_embedding):\n        super().__init__()\n        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n\n        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n                                            downscaling_factor=downscaling_factor, )\n\n        self.layers = nn.ModuleList([])\n        for _ in range(layers // 2):\n            self.layers.append(nn.ModuleList([\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n            ]))\n\n    def forward(self, x):\n        x = self.patch_partition(x)\n        for regular_block, shifted_block in self.layers:\n            x = regular_block(x)\n            x = shifted_block(x)\n            # print(x.permute(0, 3, 1, 2).shape)\n        return x.permute(0, 3, 1, 2)\n\nclass StageModule_Up(nn.Module):\n    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n                 relative_pos_embedding):\n        super().__init__()\n        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n\n        self.patch_partition = PatchExpanding(in_channels=in_channels, out_channels=hidden_dimension,\n                                            upscaling_factor=downscaling_factor)\n\n        self.layers = nn.ModuleList([])\n        for _ in range(layers // 2):\n            self.layers.append(nn.ModuleList([\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n            ]))\n\n    def forward(self, x):\n        x = self.patch_partition(x)\n        for regular_block, shifted_block in self.layers:\n            x = regular_block(x)\n            x = shifted_block(x)\n        return x.permute(0, 3, 1, 2)\n\nclass SwinTransformer(nn.Module):\n    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n        super().__init__()\n        \n        # Encoder\n        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        \n        # Bottle Neck\n        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        \n        # Decoder\n        self.stage5 = StageModule_Up(in_channels=hidden_dim * 8, hidden_dimension=hidden_dim * 4, layers=layers[3],\n                                     downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n                                     window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        # self.concat_linear5 = nn.Linear(int(hidden_dim * 2 ** 3), int(hidden_dim * 2 ** 2))\n        self.concat_l5 = torch.nn.Conv2d(768, 384, kernel_size=1)\n        \n        self.stage6 = StageModule_Up(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 2, layers=layers[2],\n                                     downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n                                     window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n        self.concat_l6 = torch.nn.Conv2d(384, 192, kernel_size=1)\n        \n        self.stage7 = StageModule_Up(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim, layers=layers[1],\n                                     downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n                                     window_size=window_size, relative_pos_embedding=relative_pos_embedding) \n        self.concat_l7 = torch.nn.Conv2d(192, 96, kernel_size=1)\n            \n        self.PFE = PatchFinalExpanding(in_channels=hidden_dim, out_channels=hidden_dim, upscaling_factor=downscaling_factors[0] )\n        \n        self.output = nn.Conv2d(in_channels=hidden_dim, out_channels=num_classes, kernel_size=1, bias=False)\n        \n        # self.deconv1 = nn.ConvTranspose2d(hidden_dim * 8, hidden_dim * 4, kernel_size=downscaling_factors[3], stride=downscaling_factors[3])\n        # self.deconv2 = nn.ConvTranspose2d(hidden_dim * 4, hidden_dim * 2, kernel_size=downscaling_factors[2], stride=downscaling_factors[2])   \n        # self.deconv3 = nn.ConvTranspose2d(hidden_dim * 2, hidden_dim, kernel_size=downscaling_factors[1], stride=downscaling_factors[1])\n        # self.deconv4 = nn.ConvTranspose2d(hidden_dim, channels, kernel_size=downscaling_factors[0], stride=downscaling_factors[0])\n        \n        \n        \n        \n        self.mlp_head = nn.Sequential(nn.LayerNorm(hidden_dim * 8), nn.Linear(hidden_dim * 8, num_classes))\n        \n        \n        # self.classifier = nn.Conv2d(3, 31, kernel_size=1)\n\n    def forward(self, img):\n        \n        # Encoder\n        x1 = self.stage1(img) # size=(N,  96, 32, 32)\n        x2 = self.stage2(x1)  # size=(N, 192, 16, 16)\n        x3 = self.stage3(x2)  # size=(N, 384,  8,  8)\n\n        # BottleNeck\n        x4 = self.stage4(x3)  # size=(N, 768,  4,  4)\n        #print(\"x4: \",x4.shape)\n        \n        # Decoder \n        x5 = self.stage5(x4) # size=(N, 384,  8,  8)\n        #print(\"x5: \",x5.shape)\n        #x5 = x5 + x3\n        x5 = torch.cat([x5, x3], dim=1)\n        x5 = self.concat_l5(x5)\n        #print(\"conx5: \",x5.shape)\n        \n        x6 = self.stage6(x5) # size=(N, 192, 16, 16)\n        #print(\"x6: \",x6.shape)\n        #x6 = x6 + x2\n        x6 = torch.cat([x6, x2], dim=1)\n        x6 = self.concat_l6(x6)\n        \n        x7 = self.stage7(x6) # size=(N,  96, 32, 32)\n        #print(\"x7: \",x7.shape)\n        #x7 = x7 + x1\n        x7 = torch.cat([x7, x1], dim=1)\n        x7 = self.concat_l7(x7)\n        #print(\"conx7: \",x7.shape)\n        x8 = self.PFE(x7).permute(0,3,1,2)\n        #print(\"x8: \",x8.shape)\n        score = self.output(x8)\n        \n        # x = x.mean(dim=[2, 3]) # for classifier \n        # return self.mlp_head(x)\n        # score = self.classifier(x7)                    # size=(N, n_class, x.H/1, x.W/1) \n\n        return score\n\n","metadata":{"_uuid":"005ec9fe-d084-40e9-aea7-0cc85c4de862","_cell_guid":"a9685701-9ccb-4715-8786-57aff038093c","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}